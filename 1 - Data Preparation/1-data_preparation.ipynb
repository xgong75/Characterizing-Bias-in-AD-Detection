{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Tw-L9Swh_-m9"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"mpW4FFH4_-nA"},"source":["## Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5Hg4j0W_-nB"},"outputs":[],"source":["# fn = '/Users/anya/Documents/ML4HC/mimic-iv-3.1/hosp/diagnoses_icd.csv.gz'\n","# count=1\n","\n","# ad_diagnosis = pd.DataFrame({'subject_id':[], 'hadm_id': [], 'icd_code':[]})\n","# adrd_codes = ['F0150', 'F0280', 'F0281', 'F0390', 'F0391', 'G300', 'G301', 'G308', 'G309', 'G3101',\n","#              'G3109', 'G3181', 'G3182', 'G3183', 'G3184', 'G3185', 'G3189', 'G319', 'R4181']\n","\n","# f1 = gzip.open(fn, 'rt')\n","# for line  in f1:\n","#     ln = line.strip().split(',')\n","#     subject_id = ln[0]\n","#     if subject_id != 'subject_id':\n","#         hadm_id = ln[1]\n","#         icd_code = ln[3]\n","#         icd_version = int(ln[4])\n","#         if icd_version == 10:\n","#             if icd_code in adrd_codes:\n","#                 new = pd.DataFrame({'subject_id': [subject_id], 'hadm_id': [hadm_id], 'icd_code': [icd_code]})\n","#                 ad_diagnosis = pd.concat([ad_diagnosis, new])\n","# f1.close()\n","# ad_diagnosis.to_csv('data/1_ad_diagnosis.csv', header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcTdzNnh_-nC"},"outputs":[],"source":["# fn = '/Users/anya/Documents/ML4HC/mimic-iv-3.1/hosp/patients.csv.gz'\n","\n","# pts = {}\n","# f1 = gzip.open(fn, 'rt')\n","# for line  in f1:\n","#     ln = line.strip().split(',')\n","#     subject_id = ln[0]\n","#     if subject_id != 'subject_id':\n","#         gender = ln[1]\n","#         anchor_age = ln[2]\n","#         anchor_year = ln[3]\n","#         pts[subject_id] = [gender, anchor_age, anchor_year]\n","# f1.close()\n","# len(pts)\n","\n","# fn = '/Users/anya/Documents/ML4HC/mimic-iv-3.1/hosp/admissions.csv.gz'\n","\n","# demographics = pd.DataFrame({'subject_id': [], 'hadm_id': [], 'admityear': [], 'admitmonth': [], 'admitday': [], 'admittime': [],\n","#                              'gender': [], 'age': [], 'admission_type': [], 'insurance': [], 'language': [], 'marital_status': [], 'race': []})\n","\n","# f1 = gzip.open(fn, 'rt')\n","# for line  in f1:\n","#     ln = line.strip().split(',')\n","#     subject_id = ln[0]\n","#     if subject_id in pts:\n","#         hadm_id = ln[1]\n","#         admittime = ln[2]\n","#         admission_type = ln[5]\n","#         insurance = ln[9]\n","#         language = ln[10]\n","#         marital_status = ln[11]\n","#         race = ln[12]\n","#         new = pd.DataFrame({'subject_id': [subject_id], 'hadm_id': [hadm_id], 'admittime': [admittime], 'admission_type': [admission_type], 'insurance': [insurance],\n","#                             'language': [language], 'marital_status': [marital_status], 'race': [race]})\n","#         new[['admitdate', 'admittime']] = new['admittime'].str.split(' ', expand=True)\n","#         new[['admityear', 'admitmonth', 'admitday']] = new['admitdate'].str.split('-', expand=True)\n","#         new = new.drop(columns='admitdate')\n","#         gender, anchor_age, anchor_year = pts[subject_id]\n","#         age = int(new['admityear'][0]) - int(anchor_year) + int(anchor_age)\n","#         # if age >= 65:\n","#         new[['gender', 'age']] = [gender, age]\n","#         demographics = pd.concat([demographics, new])\n","# f1.close()\n","# demographics.to_csv('data/2_demographics.csv', index=False, header=True)\n","# demographics\n"]},{"cell_type":"markdown","metadata":{"id":"FJaMvoQe_-nC"},"source":["## Merge Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alXCsYJW_-nC"},"outputs":[],"source":["ad_codes = ['G300', 'G301', 'G308', 'G309']\n","agg_funcs = {'adrd_icd': lambda x: ','.join(x.dropna().astype(str)),\n","             'adrd': 'max',\n","             'ad': 'max'}\n","\n","diag = pd.read_csv('data/1_ad_diagnosis.csv')\n","diag.columns = ['subject_id', 'hadm_id', 'adrd_icd']\n","diag['adrd'] = 1\n","diag['ad'] = [1 if x in ad_codes else np.nan for x in diag['adrd_icd']]\n","diag = diag.groupby(['subject_id', 'hadm_id'], as_index=False).agg(agg_funcs)\n","ad_flag = diag.groupby('subject_id')['ad'].max()\n","diag['case_status'] = diag['subject_id'].map(ad_flag)\n","print('Maximum number of unique patient (ADRD): %s' % len(diag['subject_id'].unique()))\n","temp = diag[diag['ad'] == 1]\n","print('Maximum number of unique patients (AD): %s' % len(temp['subject_id'].unique()))\n","diag"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_K5ZKnb8_-nD"},"outputs":[],"source":["race_map1 = {'WHITE': 'White',\n","            'BLACK/AFRICAN AMERICAN': 'African American',\n","            'UNKNOWN': 'Other',\n","            'WHITE - OTHER EUROPEAN': 'White',\n","            'WHITE - RUSSIAN': 'White',\n","            'OTHER': 'Other',\n","            'ASIAN - CHINESE': 'Asian',\n","            'HISPANIC/LATINO - PUERTO RICAN': 'Hispanic/Latino',\n","            'BLACK/CARIBBEAN ISLAND': 'Carribean Island',\n","            'BLACK/CAPE VERDEAN': 'Cape Verdean',\n","            'UNABLE TO OBTAIN': 'Other',\n","            'HISPANIC/LATINO - DOMINICAN': 'Hispanic/Latino',\n","            'ASIAN': 'Asian',\n","            'WHITE - EASTERN EUROPEAN': 'White',\n","            'BLACK/AFRICAN': 'African',\n","            'PORTUGUESE': 'White',\n","            'HISPANIC/LATINO - CUBAN': 'Hispanic/Latino',\n","            'ASIAN - SOUTH EAST ASIAN': 'Asian',\n","            'AMERICAN INDIAN/ALASKA NATIVE': 'Native American',\n","            'HISPANIC/LATINO - COLUMBIAN': 'Hispanic/Latino',\n","            'HISPANIC/LATINO - HONDURAN': 'Hispanic/Latino',\n","            'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'Pacific Islander',\n","            'ASIAN - ASIAN INDIAN': 'Indian',\n","            'HISPANIC/LATINO - SALVADORAN': 'Hispanic/Latino',\n","            'PATIENT DECLINED TO ANSWER': 'Other',\n","            'HISPANIC/LATINO - CENTRAL AMERICAN': 'Hispanic/Latino',\n","            'HISPANIC/LATINO - GUATEMALAN': 'Hispanic/Latino',\n","            'SOUTH AMERICAN': 'South American',\n","            'ASIAN - KOREAN': 'Asian',\n","            'HISPANIC/LATINO - MEXICAN': 'Hispanic/Latino',\n","            'WHITE - BRAZILIAN': 'White',\n","            'HISPANIC OR LATINO': 'Hispanic/Latino',\n","            'MULTIPLE RACE/ETHNICITY': 'Other'}\n","\n","# group \"black\" with African American, Indian with Asian, and all other groups with other\n","race_map2 = {'White': 'White',\n","             'African American': 'African American',\n","             'Other': 'Other',\n","             'Hispanic/Latino': 'Hispanic/Latino',\n","             'Asian': 'Asian',\n","             'Cape Verdean': 'African American',\n","             'Carribean Island': 'African American',\n","             'African': 'African American',\n","             'Indian': 'Asian',\n","             'South American': 'Other',\n","             'Native American': 'Other',\n","             'Pacific Islander': 'Other'\n","             }\n","\n","# group African with African American, Indian with Asian, and all other groups with other\n","race_map3 = {'White': 'White',\n","             'African American': 'African American',\n","             'Other': 'Other',\n","             'Hispanic/Latino': 'Hispanic/Latino',\n","             'Asian': 'Asian',\n","             'Cape Verdean': 'Other',\n","             'Carribean Island': 'Other',\n","             'African': 'African American',\n","             'Indian': 'Asian',\n","             'South American': 'Other',\n","             'Native American': 'Other',\n","             'Pacific Islander': 'Other'\n","             }\n","\n","# just group smaller groups with other\n","race_map4 = {'White': 'White',\n","             'African American': 'African American',\n","             'Other': 'Other',\n","             'Hispanic/Latino': 'Hispanic/Latino',\n","             'Asian': 'Asian',\n","             'Cape Verdean': 'Other',\n","             'Carribean Island': 'Other',\n","             'African': 'Other',\n","             'Indian': 'Other',\n","             'South American': 'Other',\n","             'Native American': 'Other',\n","             'Pacific Islander': 'Other'\n","             }\n","\n","insurance_map = {'Medicare': 'Medicare',\n","                 'Medicaid': 'Medicaid',\n","                 'Private': 'Other',\n","                 'Other': 'Other',\n","                 'No charge': 'Other',\n","                 'Unknown': 'Other'}\n","\n","demo = pd.read_csv('data/2_demographics.csv')\n","demo['marital_status'] = demo['marital_status'].fillna('UNKNOWN')\n","demo['insurance'] = demo['insurance'].fillna('Other')\n","demo['insurance_group'] = demo['insurance'].map(insurance_map)\n","demo['language'] = demo['language'].fillna('Unknown')\n","demo['language_group'] = ['English' if x == 'English' else 'Non-English' for x in demo['language']]\n","demo['race_group1'] = demo['race'].map(race_map1)\n","demo['race_group2'] = demo['race_group1'].map(race_map2)\n","demo['race_group3'] = demo['race_group1'].map(race_map3)\n","demo['race_group4'] = demo['race_group1'].map(race_map4)\n","demo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"il9JOQ_l_-nE"},"outputs":[],"source":["print(demo.shape)\n","print(diag.shape)\n","df1 = pd.merge(demo, diag, how='outer', on=['subject_id', 'hadm_id'])\n","print(len(df1[df1['ad']==1]['subject_id'].unique()))\n","df1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPEUWH2n_-nE"},"outputs":[],"source":["print(len(df1[df1['adrd']==1]['subject_id'].unique()))\n","print(len(df1[df1['ad']==1]['subject_id'].unique()))"]},{"cell_type":"markdown","metadata":{"id":"i972FmSk_-nE"},"source":["### Forward fill diagnosis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DU2xiks4_-nF"},"outputs":[],"source":["# fix situations where ICD recorded as AD for earlier visit but not later visits\n","# assume that if AD diagnosed once it is diagnosed for later visits\n","df1_sorted = df1.sort_values(by=['subject_id', 'admityear', 'admitmonth', 'admitday', 'admittime'])\n","df1_sorted['ad'] = df1_sorted.groupby('subject_id')['ad'].ffill().fillna(0).astype(int)\n","df1_sorted['adrd'] = df1_sorted.groupby('subject_id')['adrd'].ffill().fillna(0).astype(int)\n","df1_sorted['adrd_icd'] = df1['adrd_icd'].fillna('NotAD')\n","\n","ad_flag = df1_sorted.groupby('subject_id')['ad'].max()\n","df1_sorted['case_status'] = df1_sorted['subject_id'].map(ad_flag)\n","adrd_flag = df1_sorted.groupby('subject_id')['adrd'].max()\n","df1_sorted['adrd_status'] = df1_sorted['subject_id'].map(adrd_flag)\n","df1_sorted.to_csv('data/3_demographics_diagnosis.csv', index=False, header=True)\n","df1_sorted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDBJKmhP_-nF"},"outputs":[],"source":["print(len(df1_sorted[df1_sorted['adrd']==1]['subject_id'].unique()))\n","print(len(df1_sorted[df1_sorted['ad']==1]['subject_id'].unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhwzM6Po_-nF"},"outputs":[],"source":["df1_sorted.isnull().any()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK6Mzecz_-nF"},"outputs":[],"source":["df1_sorted[df1_sorted['subject_id']==16221825]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxAI_Lkr_-nF"},"outputs":[],"source":["print(len(df1_sorted[df1_sorted['adrd_status']==1]['subject_id'].unique()))\n","print(len(df1_sorted[df1_sorted['case_status']==1]['subject_id'].unique()))"]},{"cell_type":"markdown","metadata":{"id":"mG3tMGC7_-nF"},"source":["### Discharge notes"]},{"cell_type":"markdown","metadata":{"id":"3wkZk3RK_-nG"},"source":["```\n","awk 'BEGIN {FS=\",\"; OFS=\"\\t\"} /^[0-9]{4,}/ {print $1,$2,$3,$4,$5,$6,$7}' discharge.csv > discharge_meta.tsv\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41XnjNQA_-nG"},"outputs":[],"source":["# want 331,794 rows - missing 1?\n","notes = pd.read_csv('/Users/anya/Documents/ML4HC/mimic-iv-3.1/notes/discharge_meta.tsv', sep='\\t', header=None)\n","notes.columns = ['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq', 'chart_time', 'store_time']\n","notes = notes[notes['note_type'] == 'DS']\n","notes['subject_id'] = notes['subject_id'].astype(int)\n","notes['hadm_id'] = notes['hadm_id'].astype(int)\n","notes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hS7gYQ-X_-nG"},"outputs":[],"source":["df2 = pd.merge(df1_sorted, notes, how='left', on=['subject_id', 'hadm_id'])\n","df2 = df2[df2['subject_id'].isin(notes['subject_id'])]\n","df2.to_csv('data/4_demographics_diagnosis_dsnotes.csv', index=False, header=True)\n","print(len(df2[df2['case_status']==1]['subject_id'].unique()))\n","df2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2LdNbebd_-nG"},"outputs":[],"source":["df2.isnull().any()"]},{"cell_type":"markdown","metadata":{"id":"JRUE8uEN_-nG"},"source":["### ADRD vs. AD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yf5bFRbA_-nG"},"outputs":[],"source":["print(len(df2[df2['adrd_status']==1]['subject_id'].unique()))\n","print(len(df2[df2['case_status']==1]['subject_id'].unique()))"]},{"cell_type":"markdown","metadata":{"id":"LtIWDykR_-nG"},"source":["## Matching\n","\n","Remember to exclude patients with ADRD from control selection  \n","Code adapted from Sierra"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-7X5b7N_-nG"},"outputs":[],"source":["match_df = df2[['subject_id', 'hadm_id', 'admityear', 'admitmonth', 'admitday', 'admittime',\n","                'gender', 'age', 'admission_type', 'insurance_group', 'language_group', 'marital_status', 'race_group1',\n","                'adrd', 'ad', 'case_status', 'adrd_status']]\n","match_df.isnull().any()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioVTaxH4_-nG"},"outputs":[],"source":["cases = match_df[match_df['case_status']==1]\n","cases_counts = cases.groupby('subject_id')['hadm_id'].count()\n","cutoff = np.percentile(cases_counts, 95)\n","print('The 95th percentile cutoff is %s' % cutoff)\n","plt.hist(cases_counts, bins=100)\n","plt.title('Hospital admissions per patient')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9U6Q51PW_-nG"},"outputs":[],"source":["ctrls = match_df[match_df['adrd_status'] == 0]\n","ctrls_counts = ctrls.groupby('subject_id')['hadm_id'].count()\n","cutoff = np.percentile(ctrls_counts, 95)\n","print('The 95th percentile cutoff is %s' % cutoff)\n","plt.hist(ctrls_counts, bins=100)\n","plt.title('Hospital admissions per patient')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mToNuez_-nH"},"outputs":[],"source":["def select_row(group):\n","    ad1 = group[group['ad'] == 1]\n","    if not ad1.empty:\n","        return(ad1.loc[ad1['admitdate'].idxmin()])\n","    else:\n","        return(group.loc[group['admitdate'].idxmax()])\n","\n","clean_df = match_df.copy()\n","clean_df['admitdate'] = pd.to_datetime(clean_df[['admityear', 'admitmonth', 'admitday']].rename(\n","    columns={'admityear': 'year', 'admitmonth': 'month', 'admitday': 'day'}))\n","\n","# clean the cases so only contain first entry of AD\n","print('CASES')\n","cases = clean_df[match_df['case_status'] == 1]\n","print(cases.shape)\n","cases = cases.groupby('subject_id', group_keys=False).apply(select_row)\n","print(cases.shape)\n","cases_subjects = cases['subject_id'].unique()\n","print(len(cases_subjects))\n","cases['pheno'] = 1\n","\n","# ensure there are no cases in controls\n","print('CONTROLS')\n","ctrls = clean_df[match_df['adrd_status'] == 0]\n","print(ctrls.shape)\n","# ctrls = ctrls[ctrls['age']>=65]\n","# print(ctrls.shape)\n","ctrls = ctrls[~ctrls['subject_id'].isin(cases_subjects)]\n","print(ctrls.shape)\n","ctrls['pheno'] = 0\n","\n","# concat cases and controls\n","merged_df = pd.concat([cases, ctrls], ignore_index=True)\n","merged_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"haCq_ByG_-nH"},"outputs":[],"source":["treatment_col = 'pheno'\n","covariates = ['gender', 'race_group1', 'age', 'admission_type', 'marital_status', 'language_group', 'insurance_group']"]},{"cell_type":"markdown","metadata":{"id":"3mTLrt0Y_-nH"},"source":["### Before matching"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W60sCPeH_-nH"},"outputs":[],"source":["fig, axes = plt.subplots(len(covariates), 1, figsize=(8, 6 * len(covariates)))\n","\n","for i, covariate in enumerate(covariates):\n","    sns.histplot(data=merged_df, x=covariate, hue=treatment_col, kde=True, ax=axes[i], element=\"step\")\n","    axes[i].set_xlabel(covariate, fontsize=12)\n","    axes[i].set_ylabel(\"Density\", fontsize=12)\n","    axes[i].tick_params(axis='x', rotation=90) # Rotate x-axis labels vertically\n","\n","plt.tight_layout()\n","plt.savefig(f'figures/{treatment_col}_before_distribution.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GDfzK3NK_-nH"},"source":["### Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYIMB23L_-nH"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import BallTree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TL8qkhD9_-nH"},"outputs":[],"source":["df3 = merged_df[covariates]\n","df3[treatment_col]=merged_df[treatment_col]\n","\n","# Encode categorical variables\n","df3 = pd.get_dummies(df3, columns= [\"gender\", \"race_group1\",'admission_type',\"marital_status\", 'language_group', \"insurance_group\"], drop_first=True)\n","\n","# Calculate propensity scores\n","X = df3.drop(columns=[treatment_col])\n","y = df3[treatment_col]\n","\n","ps_model = LogisticRegression(max_iter=1000, random_state=42)\n","ps_model.fit(X, y)\n","df3[\"propensity_score\"] = ps_model.predict_proba(X)[:, 1]\n","df3[\"subject_id\"]=merged_df[\"subject_id\"]\n","\n","# 1:1 matching with caliper\n","treated = df3[df3[treatment_col] == 1]\n","controls = df3[df3[treatment_col] == 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBuzYYsm_-nH"},"outputs":[],"source":["def ultra_fast_1to1_matching_with_exclusion(treated, controls, caliper=0.2):\n","    \"\"\"\n","    Optimized 1:1 matching with no control reuse and exclusion of all rows\n","    with the same subject_id when a control is matched.\n","    \"\"\"\n","    # Convert to numpy arrays for speed\n","    treated_ps = treated[\"propensity_score\"].values.reshape(-1, 1)\n","    control_ps = controls[\"propensity_score\"].values.reshape(-1, 1)\n","    control_indices = controls.index.values\n","    control_subject_ids = controls[\"subject_id\"].values  # Track subject_ids\n","\n","    # Standardize\n","    scaler = StandardScaler()\n","    treated_ps_std = scaler.fit_transform(treated_ps)\n","    control_ps_std = scaler.transform(control_ps)\n","\n","    # Build BallTree for fast radius searches\n","    tree = BallTree(control_ps_std)\n","\n","    # Track used controls with boolean array (faster than set)\n","    used_controls = np.zeros(len(controls), dtype=bool)\n","    matches = []\n","\n","    # Process in random order to avoid quality bias\n","    rng = np.random.default_rng(42)\n","    treated_order = rng.permutation(len(treated))\n","\n","    for i in treated_order:\n","        ps = treated_ps_std[i]\n","        # Find all potential matches within caliper (radius search)\n","        match_indices = tree.query_radius([ps], r=caliper)[0]\n","\n","        # Mask for unused controls\n","        available_mask = ~used_controls[match_indices]\n","        available_indices = match_indices[available_mask]\n","\n","        if len(available_indices) > 0:\n","            # Find closest available control (using numpy argmin)\n","            distances = np.abs(control_ps_std[available_indices].flatten() - ps[0])\n","            best_idx = available_indices[np.argmin(distances)]\n","\n","            # Mark ALL rows with this subject_id as used\n","            matched_subject_id = control_subject_ids[best_idx]\n","            used_controls[control_subject_ids == matched_subject_id] = True\n","\n","            matches.append({\n","                'treated_idx': treated.index[i],\n","                'control_idx': control_indices[best_idx]\n","            })\n","\n","    return matches\n","\n","# Usage\n","matches = ultra_fast_1to1_matching_with_exclusion(treated, controls, caliper=0.2)\n","\n","# Create matched dataset (vectorized operations)\n","treated_matched = treated.loc[[m['treated_idx'] for m in matches]]\n","controls_matched = controls.loc[[m['control_idx'] for m in matches]]\n","matched_df = pd.concat([treated_matched, controls_matched])\n","\n","# Add pair IDs in one operation\n","matched_df['pair_id'] = np.repeat(np.arange(len(matches)), 2)\n","len(matched_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXGGT8Sd_-nH"},"outputs":[],"source":["# Check balance\n","def check_balance(df, covariates):\n","    balance = {}\n","    for var in covariates:\n","        if var in df.columns:\n","            t_mean = df[df[treatment_col] == 1][var].mean()\n","            c_mean = df[df[treatment_col] == 0][var].mean()\n","            pooled_std = np.sqrt((df[df[treatment_col] == 1][var].var() +\n","                                df[df[treatment_col] == 0][var].var()) / 2)\n","            balance[var] = abs((t_mean - c_mean) / pooled_std)\n","    return balance\n","\n","balance_report = check_balance(matched_df, covariates)\n","print(\"Balance Report (SMD < 0.1 is good):\")\n","for var, smd in balance_report.items():\n","    print(f\"{var}: {smd:.3f}\")\n","\n","print(f\"\\nMatched {len(treated_matched)} treated to {len(controls_matched)} controls\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PfyoeX-_-nI"},"outputs":[],"source":["# prompt: get the rows of df_selected with the same row index as in all_matched_data\n","\n","# Get the indices of the matched rows\n","matched_indices = matched_df.index\n","\n","# Select rows from df_selected using these indices\n","df_matched_selected = merged_df.loc[matched_indices]\n","\n","# Now df_matched_selected contains the rows from df_selected that were matched\n","# and have the same row index as in all_matched_data.\n","df_matched_selected\n","df_matched_selected.to_csv(f'data/5_matched_{treatment_col}.csv')\n","print(len(df_matched_selected))\n","df_matched_selected"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7fLd5-e_-nI"},"outputs":[],"source":["# prompt: check if there's duplicative value for subject_id\n","\n","# Assuming 'df' is your DataFrame as defined in the provided code.\n","duplicate_subjects = df_matched_selected[df_matched_selected.duplicated(subset=['subject_id'], keep=False)]\n","\n","if not duplicate_subjects.empty:\n","    print(\"Duplicate subject IDs found:\")\n","    print(duplicate_subjects[['subject_id']])\n","else:\n","    print(\"No duplicate subject IDs found.\")\n","\n","# prompt: check if df_matched_selected has duplicative index, if yes return index\n","\n","# Check for duplicate indices\n","duplicate_indices = df_matched_selected.index[df_matched_selected.index.duplicated(keep=False)]\n","\n","if not duplicate_indices.empty:\n","  print(\"Duplicated indices found:\")\n","  print(duplicate_indices)\n","else:\n","  print(\"No duplicate indices found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGnaSmcq_-nI"},"outputs":[],"source":["fig, axes = plt.subplots(len(covariates), 1, figsize=(8, 6 * len(covariates)))\n","\n","for i, covariate in enumerate(covariates):\n","    sns.histplot(data=df_matched_selected, x=covariate, hue=treatment_col, kde=True, ax=axes[i], element=\"step\")\n","    axes[i].set_xlabel(covariate, fontsize=12)\n","    axes[i].set_ylabel(\"Density\", fontsize=12)\n","    axes[i].tick_params(axis='x', rotation=90) # Rotate x-axis labels vertically\n","\n","plt.tight_layout()\n","plt.savefig(f'figures/{treatment_col}_after_distribution.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDamblQ9_-nI"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}