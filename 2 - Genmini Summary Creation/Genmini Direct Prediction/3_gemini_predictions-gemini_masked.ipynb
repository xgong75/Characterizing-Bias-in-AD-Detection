{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-_Pt6Ly2Nkv"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vgkGCQtlenF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import auth, drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1746827365328,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "3lJ64yjJl9h8",
    "outputId": "69150411-6d7a-4ee7-bbe4-3272a7402424"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfF5Zk0rmDC1"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Image,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    ")\n",
    "from vertexai.batch_prediction import BatchPredictionJob\n",
    "import json\n",
    "from google.cloud import storage\n",
    "\n",
    "# replace with project ID from Google Cloud Platform\n",
    "PROJECT_ID = \"mit-mlhc-v2\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC8MKUyhmEjr"
   },
   "outputs": [],
   "source": [
    "def extract_text(response):\n",
    "  \"\"\"Extracts text from the response dictionary, handling potential KeyError.\"\"\"\n",
    "  try:\n",
    "    return response['candidates'][0]['content']['parts'][0]['text']\n",
    "  except (KeyError, IndexError, TypeError):\n",
    "    # Handle cases where 'parts' key is missing or empty\n",
    "    print(response)\n",
    "   # Or any other appropriate default value\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaQkhvfMmHFq"
   },
   "outputs": [],
   "source": [
    "dir = '/content/drive/MyDrive/HSPH/Courses/MIT6.7930/AI Bias for AD/AI Bias AD/Gemini Prediction Model'\n",
    "suffix = 'gemini_summaries_tabular'\n",
    "infn = dir + '/summaries_predictions/13_gemini_summaries.csv'\n",
    "outfn = dir + '/14_gemini_predictions-' + suffix + '.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2890,
     "status": "ok",
     "timestamp": 1746827057195,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "007ucdO5mHAc",
    "outputId": "9193035b-fc09-4278-fa94-8a0ef0b375ec"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(infn)\n",
    "print(df.shape)\n",
    "df['case_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1746827057220,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "DMeBhqcvsiUL",
    "outputId": "1a656a4f-d8a2-4773-86b9-2de4f45aaffa"
   },
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746827057230,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "mHP4N_WQmG92",
    "outputId": "4c2b496c-b7e4-47e9-e1c4-8f5ce40a337d"
   },
   "outputs": [],
   "source": [
    "def get_demo_info(row):\n",
    "  age = row['age']\n",
    "  race = row['race_group2']\n",
    "  gender = 'female' if row['gender'] == 'F' else 'male'\n",
    "  marital_status = row['marital_status']\n",
    "\n",
    "  admission = row['admission_type']\n",
    "  insurance = row['insurance_group']\n",
    "  language = row['language_group']\n",
    "\n",
    "  stroke = 'stroke' if row['Stroke_History'] == 1 else 'No'\n",
    "  mi = 'myocardial infarction' if row['Myocardial_Infarction'] == 1 else 'No'\n",
    "  pvd = 'peripheral vascular disease' if row['Peripheral_Vascular_Disease'] == 1 else 'No'\n",
    "  cd = 'cerebrovascular disease' if row['Cerebrovascular_Disease'] == 1 else 'No'\n",
    "  diabetes = 'diabetes mellitus' if row['Diabetes_Mellitus'] == 1 else 'No'\n",
    "  cancer = 'cancer' if row['Cancer'] == 1 else 'No'\n",
    "  disease_arr = np.array([stroke, mi, pvd, cd, diabetes, cancer])\n",
    "  mask = disease_arr != 'No'\n",
    "  disease_true = disease_arr[mask]\n",
    "  disease = ', '.join(disease_true)\n",
    "\n",
    "  demo = f\"\"\"The following is the clinical notes summary for a {age} year old {race} {gender} who is {marital_status}.\n",
    "They were admitted to the hospital as {admission}, have {insurance} insurance, and are {language} speaking.\n",
    "They have the following disease history: {disease}\n",
    "  \"\"\"\n",
    "  return(demo)\n",
    "\n",
    "print(get_demo_info(df.loc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg7C-s_w2PZW"
   },
   "outputs": [],
   "source": [
    "pattern = r\"(LIKELY_AD|POSSIBLE_AD|UNLIKELY_AD)\"\n",
    "\n",
    "def calculate_metrics(data, pred, verbose=True):\n",
    "  y_true = data['case_status']\n",
    "  y_pred = data[pred]\n",
    "\n",
    "  # Compute confusion matrix\n",
    "  try:\n",
    "      tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "  except ValueError:\n",
    "      # Handle edge cases where only one class is present\n",
    "      tn = fp = fn = tp = 0\n",
    "      for actual, pred in zip(y_true, y_pred):\n",
    "          if actual == 1 and pred == 1:\n",
    "              tp += 1\n",
    "          elif actual == 1 and pred == 0:\n",
    "              fn += 1\n",
    "          elif actual == 0 and pred == 1:\n",
    "              fp += 1\n",
    "          elif actual == 0 and pred == 0:\n",
    "              tn += 1\n",
    "\n",
    "  # Compute fairness-related metrics\n",
    "  tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Sensitivity / Equal Opportunity\n",
    "  fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0  # False Positive Rate / Equalized Odds\n",
    "  precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Precision / Predictive Parity\n",
    "  sample_size = len(data)\n",
    "\n",
    "  # Print results\n",
    "  if verbose:\n",
    "    print(\"ðŸ“Š Overall Model Performance\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"TPR:       {tpr:.4f}\")\n",
    "    print(f\"FPR:       {fpr:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Sample Size:  {sample_size}\")\n",
    "  return(tpr, fpr, precision, sample_size)\n",
    "\n",
    "def bin_age(df):\n",
    "    bin_edges = [0, 70, 80, 90, float('inf')]\n",
    "    labels = ['<70', '70-80', '80-90', '>90']\n",
    "    df['age_group'] = pd.cut(df['age'], bins=bin_edges, labels=labels, right=False)\n",
    "    return df\n",
    "\n",
    "def bootstrap_metrics(y_true, y_pred, n_iterations=1000):\n",
    "    stats = {'TPR': [], 'FPR': [], 'Precision': []}\n",
    "    data = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        sample = data.sample(frac=1.0, replace=True)\n",
    "        yt = sample['y_true']\n",
    "        yp = sample['y_pred']\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(yt, yp, labels=[0, 1]).ravel()\n",
    "        except:\n",
    "            continue  # skip samples without both classes\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "\n",
    "        stats['TPR'].append(tpr)\n",
    "        stats['FPR'].append(fpr)\n",
    "        stats['Precision'].append(precision)\n",
    "\n",
    "    return {\n",
    "        'TPR': (np.nanpercentile(stats['TPR'], 2.5), np.nanpercentile(stats['TPR'], 97.5)),\n",
    "        'FPR': (np.nanpercentile(stats['FPR'], 2.5), np.nanpercentile(stats['FPR'], 97.5)),\n",
    "        'Precision': (np.nanpercentile(stats['Precision'], 2.5), np.nanpercentile(stats['Precision'], 97.5)),\n",
    "    }\n",
    "\n",
    "demo_fts = ['age_group', 'gender', 'insurance_group', 'language_group', 'race_group2', 'admission_type', 'marital_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqvRbFxnw_WO"
   },
   "source": [
    "## Using full summaries, with tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 642,
     "status": "ok",
     "timestamp": 1746827061394,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "cM8EnOPnw-xT",
    "outputId": "6ce809ee-d06c-446c-b1bb-ace9763d60f4"
   },
   "outputs": [],
   "source": [
    "dir = '/content/drive/MyDrive/HSPH/Courses/MIT6.7930/AI Bias for AD/AI Bias AD/Gemini Prediction Model'\n",
    "suffix = 'gemini_summaries_tabular'\n",
    "infn = dir + '/summaries_predictions/13_gemini_summaries.csv'\n",
    "outfn = dir + '/14_gemini_predictions-' + suffix + '.csv'\n",
    "\n",
    "df = pd.read_csv(infn)\n",
    "print(df.shape)\n",
    "df['case_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzRRsnsAmGtL"
   },
   "outputs": [],
   "source": [
    "def df_to_jsonl_gcs(df, bucket_name, blob_name):\n",
    "    \"\"\"Converts a DataFrame to JSONL and uploads to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame with a 'text' column.\n",
    "        bucket_name: Name of your Google Cloud Storage bucket.\n",
    "        blob_name: Desired name for the JSONL file on GCS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a GCS client\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    if not bucket.exists():\n",
    "        bucket.create(location='US')\n",
    "        print(f'Bucket {bucket_name} created.')\n",
    "    else:\n",
    "        print(f'Bucket {bucket_name} already exists.')\n",
    "\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Write JSONL data to a string buffer\n",
    "    jsonl_data = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        ### TO DO: Provide Gemini a prompt ##############################\n",
    "        # Edit the prompt to tell Gemini how to handle your input text\n",
    "        text = row['summary']#[:50000]\n",
    "\n",
    "        demo = get_demo_info(row)\n",
    "\n",
    "        prompt = f\"\"\"You are a neurologist tasked with diagnosing alzheimer's disease based on a summary of a patient's clinical notes.\n",
    "        Please read the following clinical notes summaries and assess whether there is documented evidence suggestive of Alzheimer's Disease (AD).\n",
    "        Consider cognitive symptoms such as progressive memory loss, disorientation, language difficulties, and behavioral changes\n",
    "        as well as test results such as cognitive assessments or neuroimaging findings indicative of brain atrophy or AD markers.\n",
    "        Do not include temporary confusion from infection, medication, or unrelated acute illness.\n",
    "\n",
    "        Respond with one of the following labels:\n",
    "        - LIKELY_AD: clear evidence consistent with Alzheimer's Disease\n",
    "        - POSSIBLE_AD: some suggestive signs, but incomplete documentation\n",
    "        - UNLIKELY_AD: No indication of Alzheimer's Disease or dementia\n",
    "\n",
    "        Also provide a 1-2 sentence justification summarizing the key evidence from the note.\n",
    "\n",
    "        {demo}\n",
    "        Clinical Notes: {text}\"\"\"\n",
    "        #################################################################\n",
    "\n",
    "        json_data = {\n",
    "            \"id\": row['subject_id'],\n",
    "            \"request\": {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\"temperature\": 0.4, \"maxOutputTokens\": 2048},\n",
    "\n",
    "            }\n",
    "        }\n",
    "        jsonl_data += json.dumps(json_data) + '\\n'\n",
    "\n",
    "    # Upload the JSONL data to GCS\n",
    "    blob.upload_from_string(jsonl_data, content_type='application/jsonl')\n",
    "    print(f\"JSONL file uploaded to gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "    return f\"gs://{bucket_name}/{blob_name}\"\n",
    "\n",
    "### TO DO: Change bucket name ##############################\n",
    "\n",
    "BUCKET_NAME = 'project_gemini_predictions'\n",
    "input_uri = df_to_jsonl_gcs(df, BUCKET_NAME, 'gemini_batch_requests.jsonl')\n",
    "\n",
    "#################################################################\n",
    "\n",
    "output_uri = f\"gs://{BUCKET_NAME}/batch-prediction/\"\n",
    "\n",
    "# Submit a batch prediction job with Gemini model\n",
    "batch_prediction_job = BatchPredictionJob.submit(\n",
    "    source_model=\"gemini-1.5-flash-001\",\n",
    "    input_dataset=input_uri,\n",
    "    output_uri_prefix=output_uri,\n",
    ")\n",
    "\n",
    "# Check job status\n",
    "print(f\"Job resource name: {batch_prediction_job.resource_name}\")\n",
    "print(f\"Model resource name with the job: {batch_prediction_job.model_name}\")\n",
    "print(f\"Job state: {batch_prediction_job.state.name}\")\n",
    "\n",
    "# Refresh the job until complete\n",
    "while not batch_prediction_job.has_ended:\n",
    "    time.sleep(5)\n",
    "    batch_prediction_job.refresh()\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_prediction_job.has_succeeded:\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_prediction_job.error}\")\n",
    "\n",
    "# Check the location of the output\n",
    "print(f\"Job output location: {batch_prediction_job.output_location}\")\n",
    "\n",
    "# Example response:\n",
    "#  Job output location: gs://your-bucket/gen-ai-batch-prediction/prediction-model-year-month-day-hour:minute:second.12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1917,
     "status": "ok",
     "timestamp": 1746827067371,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "DcF-UnQTmJLj",
    "outputId": "8339b06a-877c-4b8a-ea6e-a1256822062f"
   },
   "outputs": [],
   "source": [
    "# Load the JSONL file into a DataFrame\n",
    "# once you've made your predictions, they should be\n",
    "# stored at your google cloud storage bucket specified by the path\n",
    "# and you should be able to download it from this path\n",
    "path = 'gs://project_gemini_predictions/batch-prediction/prediction-model-2025-05-02T04:19:59.439135Z'\n",
    "output_path = path + '/predictions.jsonl'\n",
    "\n",
    "results = pd.read_json(output_path, lines=True)\n",
    "results = results.join(pd.json_normalize(results[\"response\"], \"candidates\"))\n",
    "print(results.shape)\n",
    "\n",
    "# Note some inputs may not generate predictions due to SAFETY constraints\n",
    "results['summary'] = results['response'].apply(extract_text)\n",
    "results = results[results['summary'] != '']\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1746827068267,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "qzK9_UY01SbK",
    "outputId": "666c5bd7-cde4-496e-b3c7-1953e78e33ec"
   },
   "outputs": [],
   "source": [
    "res_df = results[['id', 'summary']].copy()\n",
    "res_df.columns = ['subject_id', 'gemini_text']\n",
    "res_df['gemini_pred'] = res_df['gemini_text'].str.extract(pattern)\n",
    "res_df['pred'] = [0 if x == 'UNLIKELY_AD' else 1 for x in res_df['gemini_pred']]\n",
    "# res_df['pred_v2'] = [1 if x == 'LIKELY_AD' else 0 for x in res_df['gemini_pred']]\n",
    "print(res_df.shape)\n",
    "print(res_df['gemini_pred'].value_counts())\n",
    "print(res_df['pred'].value_counts())\n",
    "# print(res_df['pred_v2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1746827070117,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "cK6Guk663-_3",
    "outputId": "aa78026e-043b-480b-c78f-80fc0a538be9"
   },
   "outputs": [],
   "source": [
    "final = pd.merge(res_df, df, on='subject_id', how='inner')\n",
    "final = bin_age(final)\n",
    "\n",
    "print(final.shape)\n",
    "print('\\nPredictions V1:')\n",
    "tpr,fpr, precision, n = calculate_metrics(final, 'pred', True)\n",
    "print('\\nPredictions V2:')\n",
    "# tpr,fpr, precision, n = calculate_metrics(final, 'pred_v2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1746827449675,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "PteKJk_MgHfT",
    "outputId": "60a4e0b3-f18e-4456-b2d2-f689fb93faea"
   },
   "outputs": [],
   "source": [
    "likely = final[final['gemini_pred'] != 'POSSIBLE_AD'].copy()\n",
    "likely['pred'] = [0 if x == 'UNLIKELY_AD' else 1 for x in likely['gemini_pred']]\n",
    "print(likely['gemini_pred'].value_counts())\n",
    "print(likely['pred'].value_counts())\n",
    "tpr,fpr, precision, n = calculate_metrics(likely, 'pred', True)\n",
    "true_pred = sum(likely['pred'] == likely['case_status'])\n",
    "acc = true_pred/len(likely)\n",
    "print('Accuracy: %s' % round(acc,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746827073921,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "eXcOiOwSgHc-",
    "outputId": "8405cdfc-1145-4102-c64a-204945eaa75f"
   },
   "outputs": [],
   "source": [
    "possible = final[final['gemini_pred'] != 'LIKELY_AD'].copy()\n",
    "print(possible['gemini_pred'].value_counts())\n",
    "possible['pred'] = [0 if x == 'UNLIKELY_AD' else 1 for x in possible['gemini_pred']]\n",
    "print(possible['gemini_pred'].value_counts())\n",
    "print(possible['pred'].value_counts())\n",
    "tpr,fpr, precision, n = calculate_metrics(possible, 'pred', True)\n",
    "true_pred = sum(possible['pred'] == possible['case_status'])\n",
    "acc = true_pred/len(possible)\n",
    "print('Accuracy: %s' % round(acc,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1746827078975,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "es1emo5KT7CC",
    "outputId": "84d72cf2-920d-4841-ee51-352680e16dc8"
   },
   "outputs": [],
   "source": [
    "ctrl = final[final['ad'] == 0].loc[2]\n",
    "print(ctrl['gemini_text'])\n",
    "print('###')\n",
    "print(ctrl['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1746827082352,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "qC-6vKmHT6_s",
    "outputId": "0051fe36-cb2a-4f11-cab5-79de0b0071a1"
   },
   "outputs": [],
   "source": [
    "ctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yRws9tiMWV6"
   },
   "outputs": [],
   "source": [
    "outfn2 = dir + '/summaries_predictions/15_gemini_predictions-' + suffix + '.csv'\n",
    "\n",
    "final.to_csv(outfn2, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 47096,
     "status": "ok",
     "timestamp": 1746827297129,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "Ckk3C7VMAk3J",
    "outputId": "da43b81a-f7a3-4058-b459-da68860920b2"
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for group in demo_fts:\n",
    "  for value in likely[group].dropna().unique():\n",
    "    subset = likely[likely[group] == value]\n",
    "    if len(subset) > 0:\n",
    "      y_true = subset['case_status']\n",
    "      y_pred = subset['pred']\n",
    "\n",
    "      tpr, fpr, precision, sample_size = calculate_metrics(subset, 'pred', verbose=False)\n",
    "      ci = bootstrap_metrics(y_true, y_pred)\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': f\"{tpr:.3f} ({ci['TPR'][0]:.3f}â€“{ci['TPR'][1]:.3f})\",\n",
    "                      'FPR': f\"{fpr:.3f} ({ci['FPR'][0]:.3f}â€“{ci['FPR'][1]:.3f})\",\n",
    "                      'Equalized Odds (|FPR-TPR|)': f\"{(fpr-tpr):.3f} ({abs(ci['FPR'][0] - ci['TPR'][0]):.3f}â€“{abs(ci['FPR'][1] - ci['TPR'][1]):.3f})\",\n",
    "                      'Precision (Predictive Parity)': f\"{precision:.3f} ({ci['Precision'][0]:.3f}â€“{ci['Precision'][1]:.3f})\",\n",
    "                      'Sample Size': len(subset)})\n",
    "\n",
    "    else:\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': None,\n",
    "                      'Equalized Odds (|FPR-TPR|)': None,\n",
    "                      'Precision (Predictive Parity)': None,\n",
    "                      'Sample Size': 0})\n",
    "\n",
    "df_metrics = pd.DataFrame(records)\n",
    "df_metrics.set_index(['Group', 'Value'], inplace=True)\n",
    "df_metrics.to_csv(outfn, index=True, header=True)\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POcPg3XkAky8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGS9KCvnAkvC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 59803,
     "status": "ok",
     "timestamp": 1746192701154,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "0a8W13dg4Pqe",
    "outputId": "36c24225-e5e2-4920-cb43-e307b41d152e"
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for group in demo_fts:\n",
    "  for value in final[group].dropna().unique():\n",
    "    subset = final[final[group] == value]\n",
    "    if len(subset) > 0:\n",
    "      y_true = subset['case_status']\n",
    "      y_pred = subset['pred_v1']\n",
    "\n",
    "      tpr, fpr, precision, sample_size = calculate_metrics(subset, 'pred_v1', verbose=False)\n",
    "      ci = bootstrap_metrics(y_true, y_pred)\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': f\"{tpr:.3f} ({ci['TPR'][0]:.3f}â€“{ci['TPR'][1]:.3f})\",\n",
    "                      'FPR': f\"{fpr:.3f} ({ci['FPR'][0]:.3f}â€“{ci['FPR'][1]:.3f})\",\n",
    "                      'Equalized Odds (|FPR-TPR|)': f\"{(fpr-tpr):.3f} ({abs(ci['FPR'][0] - ci['TPR'][0]):.3f}â€“{abs(ci['FPR'][1] - ci['TPR'][1]):.3f})\",\n",
    "                      'Precision (Predictive Parity)': f\"{precision:.3f} ({ci['Precision'][0]:.3f}â€“{ci['Precision'][1]:.3f})\",\n",
    "                      'Sample Size': len(subset)})\n",
    "\n",
    "    else:\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': None,\n",
    "                      'Equalized Odds (|FPR-TPR|)': None,\n",
    "                      'Precision (Predictive Parity)': None,\n",
    "                      'Sample Size': 0})\n",
    "\n",
    "df_metrics = pd.DataFrame(records)\n",
    "df_metrics.set_index(['Group', 'Value'], inplace=True)\n",
    "df_metrics.to_csv(outfn, index=True, header=True)\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z4h4zKpCwAW"
   },
   "source": [
    "## Using concat summaries, with tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1746160217414,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "a9ja03USCwAX",
    "outputId": "33ccac19-1a4d-4297-ed1f-314915febc7f"
   },
   "outputs": [],
   "source": [
    "dir = '/content/drive/MyDrive/HSPH/Courses/MIT6.7930/AI Bias for AD/AI Bias AD/Gemini Prediction Model'\n",
    "suffix = 'gemini_concat_tabular'\n",
    "infn = dir + '/13_gemini_summaries.csv'\n",
    "outfn = dir + '/14_gemini_predictions-' + suffix + '.csv'\n",
    "\n",
    "df = pd.read_csv(infn)\n",
    "print(df.shape)\n",
    "df['case_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272356,
     "status": "ok",
     "timestamp": 1746160493795,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "IjS5PxP1CwAY",
    "outputId": "a955d5d2-26db-4179-9aa8-e7591f55e61a"
   },
   "outputs": [],
   "source": [
    "def df_to_jsonl_gcs(df, bucket_name, blob_name):\n",
    "    \"\"\"Converts a DataFrame to JSONL and uploads to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame with a 'text' column.\n",
    "        bucket_name: Name of your Google Cloud Storage bucket.\n",
    "        blob_name: Desired name for the JSONL file on GCS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a GCS client\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    if not bucket.exists():\n",
    "        bucket.create(location='US')\n",
    "        print(f'Bucket {bucket_name} created.')\n",
    "    else:\n",
    "        print(f'Bucket {bucket_name} already exists.')\n",
    "\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Write JSONL data to a string buffer\n",
    "    jsonl_data = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        ### TO DO: Provide Gemini a prompt ##############################\n",
    "        # Edit the prompt to tell Gemini how to handle your input text\n",
    "        text = row['concatenated_notes']#[:50000]\n",
    "\n",
    "        demo = get_demo_info(row)\n",
    "\n",
    "        prompt = f\"\"\"You are a neurologist tasked with diagnosing alzheimer's disease based on a summary of a patient's clinical notes.\n",
    "        Please read the following clinical notes summaries and assess whether there is documented evidence suggestive of Alzheimer's Disease (AD).\n",
    "        Consider cognitive symptoms such as progressive memory loss, disorientation, language difficulties, and behavioral changes\n",
    "        as well as test results such as cognitive assessments or neuroimaging findings indicative of brain atrophy or AD markers.\n",
    "        Do not include temporary confusion from infection, medication, or unrelated acute illness.\n",
    "\n",
    "        Respond with one of the following labels:\n",
    "        - LIKELY_AD: clear evidence consistent with Alzheimer's Disease\n",
    "        - POSSIBLE_AD: some suggestive signs, but incomplete documentation\n",
    "        - UNLIKELY_AD: No indication of Alzheimer's Disease or dementia\n",
    "\n",
    "        Also provide a 1-2 sentence justification summarizing the key evidence from the note.\n",
    "\n",
    "        {demo}\n",
    "        Clinical Notes: {text}\"\"\"\n",
    "        #################################################################\n",
    "\n",
    "        json_data = {\n",
    "            \"id\": row['subject_id'],\n",
    "            \"request\": {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\"temperature\": 0.4, \"maxOutputTokens\": 2048},\n",
    "\n",
    "            }\n",
    "        }\n",
    "        jsonl_data += json.dumps(json_data) + '\\n'\n",
    "\n",
    "    # Upload the JSONL data to GCS\n",
    "    blob.upload_from_string(jsonl_data, content_type='application/jsonl')\n",
    "    print(f\"JSONL file uploaded to gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "    return f\"gs://{bucket_name}/{blob_name}\"\n",
    "\n",
    "### TO DO: Change bucket name ##############################\n",
    "\n",
    "BUCKET_NAME = 'project_gemini_predictions'\n",
    "input_uri = df_to_jsonl_gcs(df, BUCKET_NAME, 'gemini_batch_requests.jsonl')\n",
    "\n",
    "#################################################################\n",
    "\n",
    "output_uri = f\"gs://{BUCKET_NAME}/batch-prediction/\"\n",
    "\n",
    "# Submit a batch prediction job with Gemini model\n",
    "batch_prediction_job = BatchPredictionJob.submit(\n",
    "    source_model=\"gemini-1.5-flash-001\",\n",
    "    input_dataset=input_uri,\n",
    "    output_uri_prefix=output_uri,\n",
    ")\n",
    "\n",
    "# Check job status\n",
    "print(f\"Job resource name: {batch_prediction_job.resource_name}\")\n",
    "print(f\"Model resource name with the job: {batch_prediction_job.model_name}\")\n",
    "print(f\"Job state: {batch_prediction_job.state.name}\")\n",
    "\n",
    "# Refresh the job until complete\n",
    "while not batch_prediction_job.has_ended:\n",
    "    time.sleep(5)\n",
    "    batch_prediction_job.refresh()\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_prediction_job.has_succeeded:\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_prediction_job.error}\")\n",
    "\n",
    "# Check the location of the output\n",
    "print(f\"Job output location: {batch_prediction_job.output_location}\")\n",
    "\n",
    "# Example response:\n",
    "#  Job output location: gs://your-bucket/gen-ai-batch-prediction/prediction-model-year-month-day-hour:minute:second.12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2202,
     "status": "ok",
     "timestamp": 1746160536838,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "5Enx6bQqCwAY",
    "outputId": "297746ba-c622-4020-c6c1-4354fa01f86f"
   },
   "outputs": [],
   "source": [
    "# Load the JSONL file into a DataFrame\n",
    "# once you've made your predictions, they should be\n",
    "# stored at your google cloud storage bucket specified by the path\n",
    "# and you should be able to download it from this path\n",
    "path = 'gs://project_gemini_predictions/batch-prediction/prediction-model-2025-05-02T04:30:23.225257Z'\n",
    "output_path = path + '/predictions.jsonl'\n",
    "\n",
    "results = pd.read_json(output_path, lines=True)\n",
    "results = results.join(pd.json_normalize(results[\"response\"], \"candidates\"))\n",
    "print(results.shape)\n",
    "\n",
    "# Note some inputs may not generate predictions due to SAFETY constraints\n",
    "results['summary'] = results['response'].apply(extract_text)\n",
    "results = results[results['summary'] != '']\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746160539112,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "Prm80_NBCwAY",
    "outputId": "ba00de9f-af86-44f4-ea52-698b78326138"
   },
   "outputs": [],
   "source": [
    "res_df = results[['id', 'summary']].copy()\n",
    "res_df.columns = ['subject_id', 'gemini_text']\n",
    "res_df['gemini_pred'] = res_df['gemini_text'].str.extract(pattern)\n",
    "res_df['pred_v1'] = [0 if x == 'UNLIKELY_AD' else 1 for x in res_df['gemini_pred']]\n",
    "res_df['pred_v2'] = [1 if x == 'LIKELY_AD' else 0 for x in res_df['gemini_pred']]\n",
    "print(res_df.shape)\n",
    "print(res_df['gemini_pred'].value_counts())\n",
    "print(res_df['pred_v1'].value_counts())\n",
    "print(res_df['pred_v2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1746160541338,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "00fHpXi7CwAY",
    "outputId": "c2b72798-d6d0-4a8e-b14c-15a50d7d665f"
   },
   "outputs": [],
   "source": [
    "final = pd.merge(res_df, df, on='subject_id', how='inner')\n",
    "final = bin_age(final)\n",
    "\n",
    "print(final.shape)\n",
    "print('\\nPredictions V1:')\n",
    "tpr,fpr, precision, n = calculate_metrics(final, 'pred_v1', True)\n",
    "print('\\nPredictions V2:')\n",
    "tpr,fpr, precision, n = calculate_metrics(final, 'pred_v2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 43639,
     "status": "ok",
     "timestamp": 1746160587276,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "sJZSBoTUCwAY",
    "outputId": "5d14bf78-ca35-470f-ec17-950372d9dd82"
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for group in demo_fts:\n",
    "  for value in final[group].dropna().unique():\n",
    "    subset = final[final[group] == value]\n",
    "    if len(subset) > 0:\n",
    "      y_true = subset['case_status']\n",
    "      y_pred = subset['pred_v1']\n",
    "\n",
    "      tpr, fpr, precision, sample_size = calculate_metrics(subset, 'pred_v1', verbose=False)\n",
    "      ci = bootstrap_metrics(y_true, y_pred)\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': f\"{tpr:.3f} ({ci['TPR'][0]:.3f}â€“{ci['TPR'][1]:.3f})\",\n",
    "                      'FPR': f\"{fpr:.3f} ({ci['FPR'][0]:.3f}â€“{ci['FPR'][1]:.3f})\",\n",
    "                      'Equalized Odds (|FPR-TPR|)': f\"{(fpr-tpr):.3f} ({abs(ci['FPR'][0] - ci['TPR'][0]):.3f}â€“{abs(ci['FPR'][1] - ci['TPR'][1]):.3f})\",\n",
    "                      'Precision (Predictive Parity)': f\"{precision:.3f} ({ci['Precision'][0]:.3f}â€“{ci['Precision'][1]:.3f})\",\n",
    "                      'Sample Size': len(subset)})\n",
    "\n",
    "    else:\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': None,\n",
    "                      'Equalized Odds (|FPR-TPR|)': None,\n",
    "                      'Precision (Predictive Parity)': None,\n",
    "                      'Sample Size': 0})\n",
    "\n",
    "df_metrics = pd.DataFrame(records)\n",
    "df_metrics.set_index(['Group', 'Value'], inplace=True)\n",
    "df_metrics.to_csv(outfn, index=False, header=True)\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc74zN40FU3z"
   },
   "source": [
    "## Using full summaries, without tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1746160587784,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "O8_X-AJBFSeL",
    "outputId": "f3bfa62f-2bc4-4844-a868-ab0411740d04"
   },
   "outputs": [],
   "source": [
    "dir = '/content/drive/MyDrive/HSPH/Courses/MIT6.7930/AI Bias for AD/AI Bias AD/Gemini Prediction Model'\n",
    "suffix = 'gemini_summaries'\n",
    "infn = dir + '/13_gemini_summaries.csv'\n",
    "outfn = dir + '/14_gemini_predictions-' + suffix + '.csv'\n",
    "\n",
    "df = pd.read_csv(infn)\n",
    "print(df.shape)\n",
    "df['case_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244034,
     "status": "ok",
     "timestamp": 1746160831819,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "ypfyRxImFSeM",
    "outputId": "8ff0bd38-116a-4b53-d9f3-9b25203a2b81"
   },
   "outputs": [],
   "source": [
    "def df_to_jsonl_gcs(df, bucket_name, blob_name):\n",
    "    \"\"\"Converts a DataFrame to JSONL and uploads to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame with a 'text' column.\n",
    "        bucket_name: Name of your Google Cloud Storage bucket.\n",
    "        blob_name: Desired name for the JSONL file on GCS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a GCS client\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    if not bucket.exists():\n",
    "        bucket.create(location='US')\n",
    "        print(f'Bucket {bucket_name} created.')\n",
    "    else:\n",
    "        print(f'Bucket {bucket_name} already exists.')\n",
    "\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Write JSONL data to a string buffer\n",
    "    jsonl_data = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        ### TO DO: Provide Gemini a prompt ##############################\n",
    "        # Edit the prompt to tell Gemini how to handle your input text\n",
    "        text = row['summary']#[:50000]\n",
    "\n",
    "        prompt = f\"\"\"You are a neurologist tasked with diagnosing alzheimer's disease based on a summary of a patient's clinical notes.\n",
    "        Please read the following clinical notes summaries and assess whether there is documented evidence suggestive of Alzheimer's Disease (AD).\n",
    "        Consider cognitive symptoms such as progressive memory loss, disorientation, language difficulties, and behavioral changes\n",
    "        as well as test results such as cognitive assessments or neuroimaging findings indicative of brain atrophy or AD markers.\n",
    "        Do not include temporary confusion from infection, medication, or unrelated acute illness.\n",
    "\n",
    "        Respond with one of the following labels:\n",
    "        - LIKELY_AD: clear evidence consistent with Alzheimer's Disease\n",
    "        - POSSIBLE_AD: some suggestive signs, but incomplete documentation\n",
    "        - UNLIKELY_AD: No indication of Alzheimer's Disease or dementia\n",
    "\n",
    "        Also provide a 1-2 sentence justification summarizing the key evidence from the note.\n",
    "\n",
    "        Clinical Notes: {text}\"\"\"\n",
    "        #################################################################\n",
    "\n",
    "        json_data = {\n",
    "            \"id\": row['subject_id'],\n",
    "            \"request\": {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\"temperature\": 0.4, \"maxOutputTokens\": 2048},\n",
    "\n",
    "            }\n",
    "        }\n",
    "        jsonl_data += json.dumps(json_data) + '\\n'\n",
    "\n",
    "    # Upload the JSONL data to GCS\n",
    "    blob.upload_from_string(jsonl_data, content_type='application/jsonl')\n",
    "    print(f\"JSONL file uploaded to gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "    return f\"gs://{bucket_name}/{blob_name}\"\n",
    "\n",
    "### TO DO: Change bucket name ##############################\n",
    "\n",
    "BUCKET_NAME = 'project_gemini_predictions'\n",
    "input_uri = df_to_jsonl_gcs(df, BUCKET_NAME, 'gemini_batch_requests.jsonl')\n",
    "\n",
    "#################################################################\n",
    "\n",
    "output_uri = f\"gs://{BUCKET_NAME}/batch-prediction/\"\n",
    "\n",
    "# Submit a batch prediction job with Gemini model\n",
    "batch_prediction_job = BatchPredictionJob.submit(\n",
    "    source_model=\"gemini-1.5-flash-001\",\n",
    "    input_dataset=input_uri,\n",
    "    output_uri_prefix=output_uri,\n",
    ")\n",
    "\n",
    "# Check job status\n",
    "print(f\"Job resource name: {batch_prediction_job.resource_name}\")\n",
    "print(f\"Model resource name with the job: {batch_prediction_job.model_name}\")\n",
    "print(f\"Job state: {batch_prediction_job.state.name}\")\n",
    "\n",
    "# Refresh the job until complete\n",
    "while not batch_prediction_job.has_ended:\n",
    "    time.sleep(5)\n",
    "    batch_prediction_job.refresh()\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_prediction_job.has_succeeded:\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_prediction_job.error}\")\n",
    "\n",
    "# Check the location of the output\n",
    "print(f\"Job output location: {batch_prediction_job.output_location}\")\n",
    "\n",
    "# Example response:\n",
    "#  Job output location: gs://your-bucket/gen-ai-batch-prediction/prediction-model-year-month-day-hour:minute:second.12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1784,
     "status": "ok",
     "timestamp": 1746160872868,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "ep3XMVleFSeM",
    "outputId": "b99af998-5b62-48f5-ee9f-aaa96fb24f87"
   },
   "outputs": [],
   "source": [
    "# Load the JSONL file into a DataFrame\n",
    "# once you've made your predictions, they should be\n",
    "# stored at your google cloud storage bucket specified by the path\n",
    "# and you should be able to download it from this path\n",
    "path = 'gs://project_gemini_predictions/batch-prediction/prediction-model-2025-05-02T04:36:29.076758Z'\n",
    "output_path = path + '/predictions.jsonl'\n",
    "\n",
    "results = pd.read_json(output_path, lines=True)\n",
    "results = results.join(pd.json_normalize(results[\"response\"], \"candidates\"))\n",
    "print(results.shape)\n",
    "\n",
    "# Note some inputs may not generate predictions due to SAFETY constraints\n",
    "results['summary'] = results['response'].apply(extract_text)\n",
    "results = results[results['summary'] != '']\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746160875719,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "k64axVvxFSeN",
    "outputId": "b9d80ae2-0f40-46a5-e779-849f929ab2f5"
   },
   "outputs": [],
   "source": [
    "res_df = results[['id', 'summary']].copy()\n",
    "res_df.columns = ['subject_id', 'gemini_text']\n",
    "res_df['gemini_pred'] = res_df['gemini_text'].str.extract(pattern)\n",
    "res_df['pred_v1'] = [0 if x == 'UNLIKELY_AD' else 1 for x in res_df['gemini_pred']]\n",
    "res_df['pred_v2'] = [1 if x == 'LIKELY_AD' else 0 for x in res_df['gemini_pred']]\n",
    "print(res_df.shape)\n",
    "print(res_df['gemini_pred'].value_counts())\n",
    "print(res_df['pred_v1'].value_counts())\n",
    "print(res_df['pred_v2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746160879120,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "3WlzLd-_FSeN",
    "outputId": "a5e667a0-c94f-4000-e562-7ca82afa2431"
   },
   "outputs": [],
   "source": [
    "final = pd.merge(res_df, df, on='subject_id', how='inner')\n",
    "final = bin_age(final)\n",
    "\n",
    "print(final.shape)\n",
    "print('\\nPredictions V1:')\n",
    "tpr,fpr, precision, n = calculate_metrics(final, 'pred_v1', True)\n",
    "print('\\nPredictions V2:')\n",
    "tpr,fpr, precision, n = calculate_metrics(final, 'pred_v2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 46309,
     "status": "ok",
     "timestamp": 1746160939009,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "heMpq9SPFSeN",
    "outputId": "7e4a2e4f-42e8-49a4-a8c3-24dbe7c24844"
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for group in demo_fts:\n",
    "  for value in final[group].dropna().unique():\n",
    "    subset = final[final[group] == value]\n",
    "    if len(subset) > 0:\n",
    "      y_true = subset['case_status']\n",
    "      y_pred = subset['pred_v1']\n",
    "\n",
    "      tpr, fpr, precision, sample_size = calculate_metrics(subset, 'pred_v1', verbose=False)\n",
    "      ci = bootstrap_metrics(y_true, y_pred)\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': f\"{tpr:.3f} ({ci['TPR'][0]:.3f}â€“{ci['TPR'][1]:.3f})\",\n",
    "                      'FPR': f\"{fpr:.3f} ({ci['FPR'][0]:.3f}â€“{ci['FPR'][1]:.3f})\",\n",
    "                      'Equalized Odds (|FPR-TPR|)': f\"{(fpr-tpr):.3f} ({abs(ci['FPR'][0] - ci['TPR'][0]):.3f}â€“{abs(ci['FPR'][1] - ci['TPR'][1]):.3f})\",\n",
    "                      'Precision (Predictive Parity)': f\"{precision:.3f} ({ci['Precision'][0]:.3f}â€“{ci['Precision'][1]:.3f})\",\n",
    "                      'Sample Size': len(subset)})\n",
    "\n",
    "    else:\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': None,\n",
    "                      'Equalized Odds (|FPR-TPR|)': None,\n",
    "                      'Precision (Predictive Parity)': None,\n",
    "                      'Sample Size': 0})\n",
    "\n",
    "df_metrics = pd.DataFrame(records)\n",
    "df_metrics.set_index(['Group', 'Value'], inplace=True)\n",
    "df_metrics.to_csv(outfn, index=False, header=True)\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOKwZROTK_Gy"
   },
   "source": [
    "## Using full notes, with tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "executionInfo": {
     "elapsed": 2849,
     "status": "ok",
     "timestamp": 1746160941857,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "6Vd1HBqcK_Gz",
    "outputId": "5319e0da-2f74-48ce-fd52-3fb22fa85916"
   },
   "outputs": [],
   "source": [
    "dir = '/content/drive/MyDrive/HSPH/Courses/MIT6.7930/AI Bias for AD/AI Bias AD/Gemini Prediction Model'\n",
    "suffix = 'gemini_all_tabular'\n",
    "infn = dir + '/13_gemini_all.csv'\n",
    "outfn = dir + '/14_gemini_predictions-' + suffix + '.csv'\n",
    "\n",
    "df = pd.read_csv(infn)\n",
    "print(df.shape)\n",
    "df['case_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423928,
     "status": "ok",
     "timestamp": 1746161365795,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "53QgKx5XK_Gz",
    "outputId": "4541a138-11ca-4b07-a58e-b0a392a6955f"
   },
   "outputs": [],
   "source": [
    "def df_to_jsonl_gcs(df, bucket_name, blob_name):\n",
    "    \"\"\"Converts a DataFrame to JSONL and uploads to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame with a 'text' column.\n",
    "        bucket_name: Name of your Google Cloud Storage bucket.\n",
    "        blob_name: Desired name for the JSONL file on GCS.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a GCS client\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    if not bucket.exists():\n",
    "        bucket.create(location='US')\n",
    "        print(f'Bucket {bucket_name} created.')\n",
    "    else:\n",
    "        print(f'Bucket {bucket_name} already exists.')\n",
    "\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Write JSONL data to a string buffer\n",
    "    jsonl_data = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        ### TO DO: Provide Gemini a prompt ##############################\n",
    "        # Edit the prompt to tell Gemini how to handle your input text\n",
    "        text = row['all_notes']#[:50000]\n",
    "\n",
    "        demo = get_demo_info(row)\n",
    "\n",
    "        prompt = f\"\"\"You are a neurologist tasked with diagnosing alzheimer's disease based on a summary of a patient's clinical notes.\n",
    "        Please read the following clinical notes summaries and assess whether there is documented evidence suggestive of Alzheimer's Disease (AD).\n",
    "        Consider cognitive symptoms such as progressive memory loss, disorientation, language difficulties, and behavioral changes\n",
    "        as well as test results such as cognitive assessments or neuroimaging findings indicative of brain atrophy or AD markers.\n",
    "        Do not include temporary confusion from infection, medication, or unrelated acute illness.\n",
    "\n",
    "        Respond with one of the following labels:\n",
    "        - LIKELY_AD: clear evidence consistent with Alzheimer's Disease\n",
    "        - POSSIBLE_AD: some suggestive signs, but incomplete documentation\n",
    "        - UNLIKELY_AD: No indication of Alzheimer's Disease or dementia\n",
    "\n",
    "        Also provide a 1-2 sentence justification summarizing the key evidence from the note.\n",
    "\n",
    "        {demo}\n",
    "\n",
    "        Clinical Notes: {text}\"\"\"\n",
    "        #################################################################\n",
    "\n",
    "        json_data = {\n",
    "            \"id\": row['subject_id'],\n",
    "            \"request\": {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [{\"text\": prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"generationConfig\": {\"temperature\": 0.4, \"maxOutputTokens\": 2048},\n",
    "\n",
    "            }\n",
    "        }\n",
    "        jsonl_data += json.dumps(json_data) + '\\n'\n",
    "\n",
    "    # Upload the JSONL data to GCS\n",
    "    blob.upload_from_string(jsonl_data, content_type='application/jsonl')\n",
    "    print(f\"JSONL file uploaded to gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "    return f\"gs://{bucket_name}/{blob_name}\"\n",
    "\n",
    "### TO DO: Change bucket name ##############################\n",
    "\n",
    "BUCKET_NAME = 'project_gemini_predictions'\n",
    "input_uri = df_to_jsonl_gcs(df, BUCKET_NAME, 'gemini_batch_requests.jsonl')\n",
    "\n",
    "#################################################################\n",
    "\n",
    "output_uri = f\"gs://{BUCKET_NAME}/batch-prediction/\"\n",
    "\n",
    "# Submit a batch prediction job with Gemini model\n",
    "batch_prediction_job = BatchPredictionJob.submit(\n",
    "    source_model=\"gemini-1.5-flash-001\",\n",
    "    input_dataset=input_uri,\n",
    "    output_uri_prefix=output_uri,\n",
    ")\n",
    "\n",
    "# Check job status\n",
    "print(f\"Job resource name: {batch_prediction_job.resource_name}\")\n",
    "print(f\"Model resource name with the job: {batch_prediction_job.model_name}\")\n",
    "print(f\"Job state: {batch_prediction_job.state.name}\")\n",
    "\n",
    "# Refresh the job until complete\n",
    "while not batch_prediction_job.has_ended:\n",
    "    time.sleep(5)\n",
    "    batch_prediction_job.refresh()\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_prediction_job.has_succeeded:\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_prediction_job.error}\")\n",
    "\n",
    "# Check the location of the output\n",
    "print(f\"Job output location: {batch_prediction_job.output_location}\")\n",
    "\n",
    "# Example response:\n",
    "#  Job output location: gs://your-bucket/gen-ai-batch-prediction/prediction-model-year-month-day-hour:minute:second.12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5044,
     "status": "ok",
     "timestamp": 1746161417287,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "SSk5eGD5K_G0",
    "outputId": "37c98803-45fe-4aff-923a-a852423a713c"
   },
   "outputs": [],
   "source": [
    "# Load the JSONL file into a DataFrame\n",
    "# once you've made your predictions, they should be\n",
    "# stored at your google cloud storage bucket specified by the path\n",
    "# and you should be able to download it from this path\n",
    "path = 'gs://project_gemini_predictions/batch-prediction/prediction-model-2025-05-02T04:42:25.287375Z'\n",
    "output_path = path + '/predictions.jsonl'\n",
    "\n",
    "results = pd.read_json(output_path, lines=True)\n",
    "results = results.join(pd.json_normalize(results[\"response\"], \"candidates\"))\n",
    "print(results.shape)\n",
    "\n",
    "# Note some inputs may not generate predictions due to SAFETY constraints\n",
    "results['summary'] = results['response'].apply(extract_text)\n",
    "results = results[results['summary'] != '']\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746161419179,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "DTeY_4HRK_G0",
    "outputId": "3b291228-c36c-4743-e1ac-7c59eebd266e"
   },
   "outputs": [],
   "source": [
    "res_df = results[['id', 'summary']].copy()\n",
    "res_df.columns = ['subject_id', 'gemini_text']\n",
    "res_df['gemini_pred'] = res_df['gemini_text'].str.extract(pattern)\n",
    "res_df['pred_v1'] = [0 if x == 'UNLIKELY_AD' else 1 for x in res_df['gemini_pred']]\n",
    "res_df['pred_v2'] = [1 if x == 'LIKELY_AD' else 0 for x in res_df['gemini_pred']]\n",
    "print(res_df.shape)\n",
    "print(res_df['gemini_pred'].value_counts())\n",
    "print(res_df['pred_v1'].value_counts())\n",
    "print(res_df['pred_v2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1746161422254,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "HsXm35XGK_G0",
    "outputId": "7bb3400f-23e3-426c-c3d1-b68263bbc288"
   },
   "outputs": [],
   "source": [
    "final = pd.merge(res_df, df, on='subject_id', how='inner')\n",
    "final = bin_age(final)\n",
    "\n",
    "print(final.shape)\n",
    "print('\\nPredictions V1:')\n",
    "tpr,fpr, precision, n = calculate_metrics(final, 'pred_v1', True)\n",
    "print('\\nPredictions V2:')\n",
    "tpr,fpr, precision, n = calculate_metrics(final, 'pred_v2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 43420,
     "status": "ok",
     "timestamp": 1746161468014,
     "user": {
      "displayName": "Anya Greenberg",
      "userId": "05297501209462370660"
     },
     "user_tz": 240
    },
    "id": "jxh5OcpEK_G0",
    "outputId": "4690dee7-fc7a-4ad7-9338-9f234f091e28"
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for group in demo_fts:\n",
    "  for value in final[group].dropna().unique():\n",
    "    subset = final[final[group] == value]\n",
    "    if len(subset) > 0:\n",
    "      y_true = subset['case_status']\n",
    "      y_pred = subset['pred_v1']\n",
    "\n",
    "      tpr, fpr, precision, sample_size = calculate_metrics(subset, 'pred_v1', verbose=False)\n",
    "      ci = bootstrap_metrics(y_true, y_pred)\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': f\"{tpr:.3f} ({ci['TPR'][0]:.3f}â€“{ci['TPR'][1]:.3f})\",\n",
    "                      'FPR': f\"{fpr:.3f} ({ci['FPR'][0]:.3f}â€“{ci['FPR'][1]:.3f})\",\n",
    "                      'Equalized Odds (|FPR-TPR|)': f\"{(fpr-tpr):.3f} ({abs(ci['FPR'][0] - ci['TPR'][0]):.3f}â€“{abs(ci['FPR'][1] - ci['TPR'][1]):.3f})\",\n",
    "                      'Precision (Predictive Parity)': f\"{precision:.3f} ({ci['Precision'][0]:.3f}â€“{ci['Precision'][1]:.3f})\",\n",
    "                      'Sample Size': len(subset)})\n",
    "\n",
    "    else:\n",
    "      records.append({'Group': group,\n",
    "                      'Value': value,\n",
    "                      'Equal Opportunity (TPR)': None,\n",
    "                      'Equalized Odds (|FPR-TPR|)': None,\n",
    "                      'Precision (Predictive Parity)': None,\n",
    "                      'Sample Size': 0})\n",
    "\n",
    "df_metrics = pd.DataFrame(records)\n",
    "df_metrics.set_index(['Group', 'Value'], inplace=True)\n",
    "df_metrics.to_csv(outfn, index=False, header=True)\n",
    "df_metrics\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMO9L5chd5h6Kavkban7U8B",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
